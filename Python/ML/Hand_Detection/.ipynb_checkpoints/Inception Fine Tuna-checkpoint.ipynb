{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "import os.path\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(base_model):\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    # and a logistic layer -- we have 2 classes\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# dimensions of our images.\n",
    "#Inception input size\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_layers_checkpoint_path = 'cp.top.best.hdf5'\n",
    "fine_tuned_checkpoint_path = 'cp.fine_tuned.best.hdf5'\n",
    "new_extended_inception_weights = 'final_weights.hdf5'\n",
    "\n",
    "train_data_dir = \"Train/\"\n",
    "validation_data_dir = \"Validate/\"\n",
    "\n",
    "nb_train_samples = len(glob.glob(train_data_dir + \"**/*.jpg\", recursive=True))\n",
    "nb_validation_samples = len(glob.glob(validation_data_dir + \"**/*.jpg\", recursive=True))\n",
    "\n",
    "top_epochs = 15\n",
    "fit_epochs = 15\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# this is the model we will train\n",
    "model = createModel(base_model)\n",
    "\n",
    "if os.path.exists(top_layers_checkpoint_path):\n",
    "    model.load_weights(top_layers_checkpoint_path)\n",
    "    print (\"Checkpoint '\" + top_layers_checkpoint_path + \"' loaded.\")\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=360,\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "#Save the model after every epoch.\n",
    "mc_top = ModelCheckpoint(top_layers_checkpoint_path, monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#Save the TensorBoard logs.\n",
    "tb = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "14/14 [==============================] - 57s 4s/step - loss: 0.9120 - acc: 0.6722 - val_loss: 3.9114 - val_acc: 0.6389\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 49s 3s/step - loss: 0.6226 - acc: 0.7060 - val_loss: 3.4320 - val_acc: 0.4792\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 53s 4s/step - loss: 0.5098 - acc: 0.7902 - val_loss: 5.2417 - val_acc: 0.6450\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 52s 4s/step - loss: 0.4804 - acc: 0.7712 - val_loss: 5.4072 - val_acc: 0.6450\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 52s 4s/step - loss: 0.5616 - acc: 0.7292 - val_loss: 5.0772 - val_acc: 0.6450\n",
      "0 input_10\n",
      "1 conv2d_847\n",
      "2 batch_normalization_847\n",
      "3 activation_847\n",
      "4 conv2d_848\n",
      "5 batch_normalization_848\n",
      "6 activation_848\n",
      "7 conv2d_849\n",
      "8 batch_normalization_849\n",
      "9 activation_849\n",
      "10 max_pooling2d_37\n",
      "11 conv2d_850\n",
      "12 batch_normalization_850\n",
      "13 activation_850\n",
      "14 conv2d_851\n",
      "15 batch_normalization_851\n",
      "16 activation_851\n",
      "17 max_pooling2d_38\n",
      "18 conv2d_855\n",
      "19 batch_normalization_855\n",
      "20 activation_855\n",
      "21 conv2d_853\n",
      "22 conv2d_856\n",
      "23 batch_normalization_853\n",
      "24 batch_normalization_856\n",
      "25 activation_853\n",
      "26 activation_856\n",
      "27 average_pooling2d_82\n",
      "28 conv2d_852\n",
      "29 conv2d_854\n",
      "30 conv2d_857\n",
      "31 conv2d_858\n",
      "32 batch_normalization_852\n",
      "33 batch_normalization_854\n",
      "34 batch_normalization_857\n",
      "35 batch_normalization_858\n",
      "36 activation_852\n",
      "37 activation_854\n",
      "38 activation_857\n",
      "39 activation_858\n",
      "40 mixed0\n",
      "41 conv2d_862\n",
      "42 batch_normalization_862\n",
      "43 activation_862\n",
      "44 conv2d_860\n",
      "45 conv2d_863\n",
      "46 batch_normalization_860\n",
      "47 batch_normalization_863\n",
      "48 activation_860\n",
      "49 activation_863\n",
      "50 average_pooling2d_83\n",
      "51 conv2d_859\n",
      "52 conv2d_861\n",
      "53 conv2d_864\n",
      "54 conv2d_865\n",
      "55 batch_normalization_859\n",
      "56 batch_normalization_861\n",
      "57 batch_normalization_864\n",
      "58 batch_normalization_865\n",
      "59 activation_859\n",
      "60 activation_861\n",
      "61 activation_864\n",
      "62 activation_865\n",
      "63 mixed1\n",
      "64 conv2d_869\n",
      "65 batch_normalization_869\n",
      "66 activation_869\n",
      "67 conv2d_867\n",
      "68 conv2d_870\n",
      "69 batch_normalization_867\n",
      "70 batch_normalization_870\n",
      "71 activation_867\n",
      "72 activation_870\n",
      "73 average_pooling2d_84\n",
      "74 conv2d_866\n",
      "75 conv2d_868\n",
      "76 conv2d_871\n",
      "77 conv2d_872\n",
      "78 batch_normalization_866\n",
      "79 batch_normalization_868\n",
      "80 batch_normalization_871\n",
      "81 batch_normalization_872\n",
      "82 activation_866\n",
      "83 activation_868\n",
      "84 activation_871\n",
      "85 activation_872\n",
      "86 mixed2\n",
      "87 conv2d_874\n",
      "88 batch_normalization_874\n",
      "89 activation_874\n",
      "90 conv2d_875\n",
      "91 batch_normalization_875\n",
      "92 activation_875\n",
      "93 conv2d_873\n",
      "94 conv2d_876\n",
      "95 batch_normalization_873\n",
      "96 batch_normalization_876\n",
      "97 activation_873\n",
      "98 activation_876\n",
      "99 max_pooling2d_39\n",
      "100 mixed3\n",
      "101 conv2d_881\n",
      "102 batch_normalization_881\n",
      "103 activation_881\n",
      "104 conv2d_882\n",
      "105 batch_normalization_882\n",
      "106 activation_882\n",
      "107 conv2d_878\n",
      "108 conv2d_883\n",
      "109 batch_normalization_878\n",
      "110 batch_normalization_883\n",
      "111 activation_878\n",
      "112 activation_883\n",
      "113 conv2d_879\n",
      "114 conv2d_884\n",
      "115 batch_normalization_879\n",
      "116 batch_normalization_884\n",
      "117 activation_879\n",
      "118 activation_884\n",
      "119 average_pooling2d_85\n",
      "120 conv2d_877\n",
      "121 conv2d_880\n",
      "122 conv2d_885\n",
      "123 conv2d_886\n",
      "124 batch_normalization_877\n",
      "125 batch_normalization_880\n",
      "126 batch_normalization_885\n",
      "127 batch_normalization_886\n",
      "128 activation_877\n",
      "129 activation_880\n",
      "130 activation_885\n",
      "131 activation_886\n",
      "132 mixed4\n",
      "133 conv2d_891\n",
      "134 batch_normalization_891\n",
      "135 activation_891\n",
      "136 conv2d_892\n",
      "137 batch_normalization_892\n",
      "138 activation_892\n",
      "139 conv2d_888\n",
      "140 conv2d_893\n",
      "141 batch_normalization_888\n",
      "142 batch_normalization_893\n",
      "143 activation_888\n",
      "144 activation_893\n",
      "145 conv2d_889\n",
      "146 conv2d_894\n",
      "147 batch_normalization_889\n",
      "148 batch_normalization_894\n",
      "149 activation_889\n",
      "150 activation_894\n",
      "151 average_pooling2d_86\n",
      "152 conv2d_887\n",
      "153 conv2d_890\n",
      "154 conv2d_895\n",
      "155 conv2d_896\n",
      "156 batch_normalization_887\n",
      "157 batch_normalization_890\n",
      "158 batch_normalization_895\n",
      "159 batch_normalization_896\n",
      "160 activation_887\n",
      "161 activation_890\n",
      "162 activation_895\n",
      "163 activation_896\n",
      "164 mixed5\n",
      "165 conv2d_901\n",
      "166 batch_normalization_901\n",
      "167 activation_901\n",
      "168 conv2d_902\n",
      "169 batch_normalization_902\n",
      "170 activation_902\n",
      "171 conv2d_898\n",
      "172 conv2d_903\n",
      "173 batch_normalization_898\n",
      "174 batch_normalization_903\n",
      "175 activation_898\n",
      "176 activation_903\n",
      "177 conv2d_899\n",
      "178 conv2d_904\n",
      "179 batch_normalization_899\n",
      "180 batch_normalization_904\n",
      "181 activation_899\n",
      "182 activation_904\n",
      "183 average_pooling2d_87\n",
      "184 conv2d_897\n",
      "185 conv2d_900\n",
      "186 conv2d_905\n",
      "187 conv2d_906\n",
      "188 batch_normalization_897\n",
      "189 batch_normalization_900\n",
      "190 batch_normalization_905\n",
      "191 batch_normalization_906\n",
      "192 activation_897\n",
      "193 activation_900\n",
      "194 activation_905\n",
      "195 activation_906\n",
      "196 mixed6\n",
      "197 conv2d_911\n",
      "198 batch_normalization_911\n",
      "199 activation_911\n",
      "200 conv2d_912\n",
      "201 batch_normalization_912\n",
      "202 activation_912\n",
      "203 conv2d_908\n",
      "204 conv2d_913\n",
      "205 batch_normalization_908\n",
      "206 batch_normalization_913\n",
      "207 activation_908\n",
      "208 activation_913\n",
      "209 conv2d_909\n",
      "210 conv2d_914\n",
      "211 batch_normalization_909\n",
      "212 batch_normalization_914\n",
      "213 activation_909\n",
      "214 activation_914\n",
      "215 average_pooling2d_88\n",
      "216 conv2d_907\n",
      "217 conv2d_910\n",
      "218 conv2d_915\n",
      "219 conv2d_916\n",
      "220 batch_normalization_907\n",
      "221 batch_normalization_910\n",
      "222 batch_normalization_915\n",
      "223 batch_normalization_916\n",
      "224 activation_907\n",
      "225 activation_910\n",
      "226 activation_915\n",
      "227 activation_916\n",
      "228 mixed7\n",
      "229 conv2d_919\n",
      "230 batch_normalization_919\n",
      "231 activation_919\n",
      "232 conv2d_920\n",
      "233 batch_normalization_920\n",
      "234 activation_920\n",
      "235 conv2d_917\n",
      "236 conv2d_921\n",
      "237 batch_normalization_917\n",
      "238 batch_normalization_921\n",
      "239 activation_917\n",
      "240 activation_921\n",
      "241 conv2d_918\n",
      "242 conv2d_922\n",
      "243 batch_normalization_918\n",
      "244 batch_normalization_922\n",
      "245 activation_918\n",
      "246 activation_922\n",
      "247 max_pooling2d_40\n",
      "248 mixed8\n",
      "249 conv2d_927\n",
      "250 batch_normalization_927\n",
      "251 activation_927\n",
      "252 conv2d_924\n",
      "253 conv2d_928\n",
      "254 batch_normalization_924\n",
      "255 batch_normalization_928\n",
      "256 activation_924\n",
      "257 activation_928\n",
      "258 conv2d_925\n",
      "259 conv2d_926\n",
      "260 conv2d_929\n",
      "261 conv2d_930\n",
      "262 average_pooling2d_89\n",
      "263 conv2d_923\n",
      "264 batch_normalization_925\n",
      "265 batch_normalization_926\n",
      "266 batch_normalization_929\n",
      "267 batch_normalization_930\n",
      "268 conv2d_931\n",
      "269 batch_normalization_923\n",
      "270 activation_925\n",
      "271 activation_926\n",
      "272 activation_929\n",
      "273 activation_930\n",
      "274 batch_normalization_931\n",
      "275 activation_923\n",
      "276 mixed9_0\n",
      "277 concatenate_19\n",
      "278 activation_931\n",
      "279 mixed9\n",
      "280 conv2d_936\n",
      "281 batch_normalization_936\n",
      "282 activation_936\n",
      "283 conv2d_933\n",
      "284 conv2d_937\n",
      "285 batch_normalization_933\n",
      "286 batch_normalization_937\n",
      "287 activation_933\n",
      "288 activation_937\n",
      "289 conv2d_934\n",
      "290 conv2d_935\n",
      "291 conv2d_938\n",
      "292 conv2d_939\n",
      "293 average_pooling2d_90\n",
      "294 conv2d_932\n",
      "295 batch_normalization_934\n",
      "296 batch_normalization_935\n",
      "297 batch_normalization_938\n",
      "298 batch_normalization_939\n",
      "299 conv2d_940\n",
      "300 batch_normalization_932\n",
      "301 activation_934\n",
      "302 activation_935\n",
      "303 activation_938\n",
      "304 activation_939\n",
      "305 batch_normalization_940\n",
      "306 activation_932\n",
      "307 mixed9_1\n",
      "308 concatenate_20\n",
      "309 activation_940\n",
      "310 mixed10\n",
      "Checkpoint 'cp.fine_tuned.best.hdf5' loaded.\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.5765 - acc: 0.7667 - val_loss: 2.0586 - val_acc: 0.4878\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 56s 4s/step - loss: 0.5862 - acc: 0.7691 - val_loss: 2.2570 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 57s 4s/step - loss: 0.4900 - acc: 0.8119 - val_loss: 2.4977 - val_acc: 0.5911\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 57s 4s/step - loss: 0.4397 - acc: 0.8249 - val_loss: 2.6293 - val_acc: 0.6016\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 58s 4s/step - loss: 0.3143 - acc: 0.8990 - val_loss: 2.9048 - val_acc: 0.6059\n"
     ]
    }
   ],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "#model.fit_generator(...)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=top_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=[mc_top, tb])\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "\n",
    "#Save the model after every epoch.\n",
    "mc_fit = ModelCheckpoint(fine_tuned_checkpoint_path, monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "\n",
    "if os.path.exists(fine_tuned_checkpoint_path):\n",
    "    model.load_weights(fine_tuned_checkpoint_path)\n",
    "    print (\"Checkpoint '\" + fine_tuned_checkpoint_path + \"' loaded.\")\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 172 layers and unfreeze the rest:\n",
    "for layer in model.layers[:172]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[172:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "#model.fit_generator(...)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=fit_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=[mc_fit, tb])\n",
    "\n",
    "model.save_weights(new_extended_inception_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test/356585_random.jpg', 'Test/959265_random.jpg', 'Test/313478_hand.jpg', 'Test/549947_random.jpg', 'Test/894056_hand.jpg', 'Test/162327_random.jpg', 'Test/956365_hand.jpg', 'Test/289946_hand.jpg', 'Test/25350_hand.jpg', 'Test/191090_random.jpg', 'Test/214575_hand.jpg', 'Test/830448_hand.jpg', 'Test/485557_random.jpg', 'Test/197863_hand.jpg', 'Test/487238_hand.jpg', 'Test/779591_hand.jpg', 'Test/925606_random.jpg', 'Test/785554_hand.jpg', 'Test/674798_hand.jpg', 'Test/897723_random.jpg', 'Test/50924_hand.jpg', 'Test/991940_hand.jpg']\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "filenames = glob.glob(\"Test/\" + \"*.jpg\")\n",
    "# np.random.shuffle(filenames)\n",
    "print(filenames)\n",
    "for i in filenames:\n",
    "    im = np.asarray(cv2.imread(i))\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)*(1./255)\n",
    "    test.append(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2835, 0.7165],\n",
       "       [0.4822, 0.5178],\n",
       "       [0.5233, 0.4767],\n",
       "       [0.1986, 0.8014],\n",
       "       [0.7097, 0.2903],\n",
       "       [0.2609, 0.7391],\n",
       "       [0.6379, 0.3621],\n",
       "       [0.3823, 0.6177],\n",
       "       [0.8361, 0.1639],\n",
       "       [0.3844, 0.6156],\n",
       "       [0.8988, 0.1012],\n",
       "       [0.8863, 0.1137],\n",
       "       [0.0319, 0.9681],\n",
       "       [0.5978, 0.4022],\n",
       "       [0.6921, 0.3079],\n",
       "       [0.6415, 0.3585],\n",
       "       [0.1692, 0.8308],\n",
       "       [0.8487, 0.1513],\n",
       "       [0.393 , 0.607 ],\n",
       "       [0.3484, 0.6516]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "np.round(model.predict(np.array(test[0:20])), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
